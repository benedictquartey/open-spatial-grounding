{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osg.utils.polyform_utils import *\n",
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from scipy.spatial.transform import Rotation as R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimpConvertor():\n",
    "    \"\"\"Converts Polycam data to the format expected by Instant-NGP\"\"\"\n",
    "    def __init__(self, corrected_image_padding: int = 5):\n",
    "        self.corrected_image_padding = corrected_image_padding\n",
    "        self.observation_data = {'images':{},\n",
    "        'poses':{},\n",
    "        'depth_data':{},\n",
    "        'rep_pose':{}}\n",
    "\n",
    "    def convert(self, folder: CaptureFolder, output_path: str = \"limp_dataset\"):\n",
    "        \"\"\"\n",
    "        Converts a Polycam CaptureFolder into a format LIMP can consume [images, depths and poses]\n",
    "\n",
    "        Args:\n",
    "            folder: the capture folder to convert\n",
    "            output_path: the path to write the transforms.json file. By default this is\n",
    "                written at the root of the CaptureFolder's data directory.\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        keyframes = folder.get_keyframes(rotate=True)\n",
    "        if len(keyframes) == 0:\n",
    "            logger.warning(\"Capture folder does not have any data! Aborting conversion to Instant NGP\")\n",
    "            return\n",
    "\n",
    "        bbox = CaptureFolder.camera_bbox(keyframes)\n",
    "        print(bbox)\n",
    "        ## Use camera bbox to compute scale, and offset \n",
    "        ## See https://github.com/NVlabs/instant-ngp/blob/master/docs/nerf_dataset_tips.md#scaling-existing-datasets\n",
    "        max_size = np.max(bbox.size()) * 0.75\n",
    "        data[\"scale\"] = float(1.0 / max_size)\n",
    "        offset = -bbox.center() / max_size\n",
    "        data[\"offset\"] = [float(offset[0]) + 0.5, float(offset[1]) + 0.5, float(offset[2]) + 0.5]\n",
    "        data[\"aabb_scale\"] = 2\n",
    "\n",
    "        ## add the frames\n",
    "        frames = []\n",
    "        for keyframe in tqdm.tqdm(keyframes):\n",
    "            frames.append(self._convert_keyframe(keyframe, folder))\n",
    "\n",
    "        data[\"frames\"] = frames\n",
    "        print(\"done\")\n",
    "        return data\n",
    "\n",
    "\n",
    "    def _convert_keyframe(self, keyframe: Keyframe, folder: CaptureFolder) -> dict:\n",
    "        \"\"\" Converts Polycam keyframe into a dictionary to be serialized as json \"\"\"\n",
    "        frame = {}\n",
    "        frame[\"id\"] = keyframe.timestamp\n",
    "\n",
    "        #add frame camera intrinsics\n",
    "        cam = keyframe.camera\n",
    "        frame[\"fl_x\"] = cam.fx\n",
    "        frame[\"fl_y\"] = cam.fy\n",
    "        if folder.has_optimized_poses():\n",
    "            # For optimized data we apply a crop of the image data, and we need to update the intrinsics here to match\n",
    "            frame[\"cx\"] = cam.cx - self.corrected_image_padding\n",
    "            frame[\"cy\"] = cam.cy - self.corrected_image_padding\n",
    "            frame[\"w\"] = cam.width - 2 * self.corrected_image_padding\n",
    "            frame[\"h\"] = cam.height - 2 * self.corrected_image_padding\n",
    "        else:\n",
    "            frame[\"cx\"] = cam.cx \n",
    "            frame[\"cy\"] = cam.cy\n",
    "            frame[\"w\"] = cam.width\n",
    "            frame[\"h\"] = cam.height\n",
    "\n",
    "        #add depth data\n",
    "        frame[\"depth_map_path\"] = \"./{}/{}.png\".format(CaptureArtifact.DEPTH_MAPS.value, keyframe.timestamp)\n",
    "        \n",
    "        # iphone Polycam depth data is stored in lossless .png format as a single channel image where depth is encoded as a 16-bit integer with units of millimeters, so a value of 1250 would correspond to 1.25 meters.\n",
    "        depth_path = os.path.join(folder.root, CaptureArtifact.DEPTH_MAPS.value, \"{}.png\".format(keyframe.timestamp))\n",
    "        depth_img = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n",
    "        frame[\"depth_image\"] = depth_img\n",
    "\n",
    "        #add frame pose data\n",
    "        frame_camera_pose = keyframe.camera.get_pose()\n",
    "        frame[\"position_x\"] = frame_camera_pose['position_x']\n",
    "        frame[\"position_y\"] = frame_camera_pose['position_y']\n",
    "        frame[\"position_z\"] = frame_camera_pose['position_z']\n",
    "        frame[\"quaternion_x\"] = frame_camera_pose['quaternion_x']\n",
    "        frame[\"quaternion_y\"] = frame_camera_pose['quaternion_y']\n",
    "        frame[\"quaternion_z\"] = frame_camera_pose['quaternion_z']\n",
    "        frame[\"quaternion_w\"] = frame_camera_pose['quaternion_w']\n",
    "\n",
    "        # add frame image\n",
    "        if folder.has_optimized_poses():\n",
    "            # For the corrected images we apply an image crop to remove the black strip around the boundary that is a result of the undistortion process\n",
    "            img_full_path = os.path.join(folder.root, CaptureArtifact.CORRECTED_IMAGES.value, \"{}.jpg\".format(keyframe.timestamp))\n",
    "            img_full_path_crop = os.path.join(folder.root, CaptureArtifact.CORRECTED_IMAGES.value, \"{}_crop.jpg\".format(keyframe.timestamp))\n",
    "            im = Image.open(img_full_path)\n",
    "            im = im.crop((self.corrected_image_padding, self.corrected_image_padding, keyframe.camera.width - 2 * self.corrected_image_padding, keyframe.camera.height - 2 * self.corrected_image_padding))\n",
    "            im.save(img_full_path_crop)\n",
    "            frame[\"image_file_path\"] = \"./{}/{}_crop.jpg\".format(CaptureArtifact.CORRECTED_IMAGES.value, keyframe.timestamp)\n",
    "            frame[\"image\"] = np.asarray(im)\n",
    "        else:\n",
    "            frame[\"file_path\"] = \"./{}/{}.jpg\".format(CaptureArtifact.IMAGES.value, keyframe.timestamp)\n",
    "            frame[\"image\"] = np.asarray(Image.open(os.path.join(folder.root, CaptureArtifact.IMAGES.value, \"{}.jpg\".format(keyframe.timestamp))))\n",
    "        if keyframe.camera.blur_score:\n",
    "            frame[\"sharpness\"] = keyframe.camera.blur_score\n",
    "        frame[\"transform_matrix\"] = keyframe.camera.transform_rows\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = \"../data/polyform_sample_data2\"\n",
    "folder = CaptureFolder(data_folder_path)\n",
    "convertor = LimpConvertor()\n",
    "transformed_data = convertor.convert(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view sample image\n",
    "data_frame = transformed_data['frames'][0]\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot image and dept map side by side\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(data_frame['image'])\n",
    "plt.title(\"Image\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(data_frame['depth_image'])\n",
    "plt.title(\"Depth Map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exposed to compute rotation matrix from quaternion\n",
    "def rotation_matrix_from_quaternion_manual(quaternion):\n",
    "   #quaternion format: (qw, qx, qy, qz)\n",
    "\n",
    "    # Step 1: Normalize the quaternion\n",
    "   quaternion = quaternion / np.linalg.norm(quaternion)\n",
    "    \n",
    "   # Step 2: Extract quaternion components\n",
    "   w, x, y, z = quaternion                              \n",
    "\n",
    "   # Step 3: Construct rotation matrix\n",
    "   R = np.array([[1 - 2 * y ** 2 - 2 * z ** 2, 2 * x * y - 2 * w * z, 2 * x * z + 2 * w * y],\n",
    "               [2 * x * y + 2 * w * z, 1 - 2 * x ** 2 - 2 * z ** 2, 2 * y * z - 2 * w * x],\n",
    "               [2 * x * z - 2 * w * y, 2 * y * z + 2 * w * x, 1 - 2 * x ** 2 - 2 * y ** 2]])\n",
    "   return R\n",
    "\n",
    "\n",
    "def rotation_matrix_from_quaternion(quaternion):\n",
    "    #quartanion format: (qx, qy, qz, qw)\n",
    "    r = R.from_quat(quaternion)\n",
    "    return r.as_matrix()\n",
    "\n",
    "def read_images_and_params(transformed_data, frame=0):\n",
    "    depth_img = transformed_data['frames'][frame]['depth_image']\n",
    "    depth_img = np.nan_to_num(depth_img, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    color_img = transformed_data['frames'][frame]['image']\n",
    "    \n",
    "    CX = transformed_data['frames'][frame]['cx']\n",
    "    CY = transformed_data['frames'][frame]['cy']\n",
    "    FX = transformed_data['frames'][frame]['fl_x']\n",
    "    FY = transformed_data['frames'][frame]['fl_y']\n",
    "    \n",
    "    # quart = [transformed_data['frames'][frame]['quaternion_x'],\n",
    "    #          transformed_data['frames'][frame]['quaternion_y'],\n",
    "    #          transformed_data['frames'][frame]['quaternion_z'],\n",
    "    #          transformed_data['frames'][frame]['quaternion_w']]\n",
    "\n",
    "    quart = [transformed_data['frames'][frame]['quaternion_w'],\n",
    "             transformed_data['frames'][frame]['quaternion_x'],\n",
    "             transformed_data['frames'][frame]['quaternion_y'],\n",
    "             transformed_data['frames'][frame]['quaternion_z']]\n",
    "    \n",
    "    datapoint_rotation_matrix = rotation_matrix_from_quaternion(quart)\n",
    "    datapoint_position = np.array([transformed_data['frames'][frame]['position_x'],\n",
    "                                   transformed_data['frames'][frame]['position_y'],\n",
    "                                   transformed_data['frames'][frame]['position_z']])\n",
    "    \n",
    "    # affine_matrix = np.array(\n",
    "    #     [\n",
    "    #         [1, 0, 0, 0],\n",
    "    #         [0, -1, 0, 0],\n",
    "    #         [0, 0, -1, 0],\n",
    "    #         [0, 0, 0, 1],\n",
    "    #     ]\n",
    "    # )\n",
    "    # datapoint_position = datapoint_position @ affine_matrix\n",
    "\n",
    "    # # Converts poses from (X, Z, Y) to (X, -Y, Z).\n",
    "    # affine_matrix = np.array(\n",
    "    #     [\n",
    "    #         [1, 0, 0, 0],\n",
    "    #         [0, 0, -1, 0],\n",
    "    #         [0, 1, 0, 0],\n",
    "    #         [0, 0, 0, 1],\n",
    "    #     ]\n",
    "    # )\n",
    "    # datapoint_position = affine_matrix @ datapoint_position\n",
    "    \n",
    "    return depth_img, color_img, CX, CY, FX, FY, datapoint_rotation_matrix, datapoint_position\n",
    "\n",
    "def resize_color_image_and_scale_intrinsics(color_img, depth_img, CX, CY, FX, FY):\n",
    "    H, W = depth_img.shape\n",
    "    H_orig, W_orig = color_img.shape[:2]\n",
    "    \n",
    "    scale_x = W / W_orig\n",
    "    scale_y = H / H_orig\n",
    "    \n",
    "    CX_new = CX * scale_x\n",
    "    CY_new = CY * scale_y\n",
    "    FX_new = FX * scale_x\n",
    "    FY_new = FY * scale_y\n",
    "    \n",
    "    if H != H_orig or W != W_orig:\n",
    "        color_img = cv2.resize(color_img, (W, H), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    return color_img, CX_new, CY_new, FX_new, FY_new\n",
    "\n",
    "def backproject_worldframe(i, j, depth, cx, cy, fx, fy, rotation_matrix, position):\n",
    "    z = depth / 1000.0  # Convert millimeters to meters\n",
    "    x = (j - cx) * z / fx\n",
    "    y = (i - cy) * z / fy\n",
    "    point_camera_frame = np.array([x, y, z])\n",
    "    point_world_frame = rotation_matrix @ point_camera_frame\n",
    "    return point_world_frame, point_camera_frame\n",
    "\n",
    "def generate_point_cloud(depth_img, color_img, CX, CY, FX, FY, rotation_matrix, position):\n",
    "    H, W = depth_img.shape\n",
    "    total_pcds = []\n",
    "    total_colors = []\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            if depth_img[i, j] > 0:  # Only consider points with valid depth\n",
    "                transformed_xyz, _ = backproject_worldframe(i, j, depth_img[i, j], CX, CY, FX, FY, rotation_matrix, position)\n",
    "                total_pcds.append(transformed_xyz)\n",
    "                total_colors.append(color_img[i, j] / 255.0)\n",
    "    return total_pcds, total_colors\n",
    "\n",
    "def fuse_point_clouds(all_pcds, all_colors):\n",
    "    fused_pcd = o3d.geometry.PointCloud()\n",
    "    fused_pcd.points = o3d.utility.Vector3dVector(all_pcds)\n",
    "    fused_pcd.colors = o3d.utility.Vector3dVector(all_colors)\n",
    "    return fused_pcd\n",
    "\n",
    "def fuse_point_clouds_with_voxel_filter(all_pcds, all_colors, voxel_size=0.01):\n",
    "    # Create an empty point cloud\n",
    "    fused_pcd = o3d.geometry.PointCloud()\n",
    "    fused_pcd.points = o3d.utility.Vector3dVector(all_pcds)\n",
    "    fused_pcd.colors = o3d.utility.Vector3dVector(all_colors)\n",
    "    \n",
    "    # Apply voxel grid filter to downsample and merge points\n",
    "    fused_pcd = fused_pcd.voxel_down_sample(voxel_size=voxel_size)\n",
    "    \n",
    "    return fused_pcd\n",
    "\n",
    "def save_point_cloud(point_cloud, filename=\"fused_pointcloud.ply\"):\n",
    "    o3d.io.write_point_cloud(filename, point_cloud)\n",
    "    print(f\"Point cloud saved to {filename}\")\n",
    "\n",
    "def save_single_point_cloud(total_pcds, total_colors, filename=\"single_pointcloud.ply\"):\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "    point_cloud.points = o3d.utility.Vector3dVector(total_pcds)\n",
    "    point_cloud.colors = o3d.utility.Vector3dVector(total_colors)\n",
    "    o3d.io.write_point_cloud(filename, point_cloud)\n",
    "    print(f\"Point cloud saved to {filename}\")\n",
    "\n",
    "## generate and save one pointcloud\n",
    "# frame = 0\n",
    "# depth_img, color_img, CX, CY, FX, FY, rotation_matrix, position = read_images_and_params(transformed_data, frame=frame)\n",
    "# color_img, CX, CY, FX, FY = resize_color_image_and_scale_intrinsics(color_img, depth_img, CX, CY, FX, FY)\n",
    "# pcds, colors = generate_point_cloud(depth_img, color_img, CX, CY, FX, FY, rotation_matrix, position)\n",
    "# save_single_point_cloud(pcds,colors)\n",
    "\n",
    "## generate and save fused pointcloud\n",
    "## Main execution\n",
    "all_pcds = []\n",
    "all_colors = []\n",
    "\n",
    "for frame in tqdm.tqdm((range(len(transformed_data['frames'])))):\n",
    "    depth_img, color_img, CX, CY, FX, FY, rotation_matrix, position = read_images_and_params(transformed_data, frame=frame)\n",
    "    color_img, CX, CY, FX, FY = resize_color_image_and_scale_intrinsics(color_img, depth_img, CX, CY, FX, FY)\n",
    "    pcds, colors = generate_point_cloud(depth_img, color_img, CX, CY, FX, FY, rotation_matrix, position)\n",
    "    ## Visualize individual frame point cloud (optional for debugging)\n",
    "    # visualize_point_cloud(pcds, colors, title=f\"Frame {frame_index} Point Cloud\")\n",
    "    all_pcds.extend(pcds)\n",
    "    all_colors.extend(colors)\n",
    "\n",
    "# fused_pcd = fuse_point_clouds(all_pcds, all_colors)\n",
    "fused_pcd = fuse_point_clouds_with_voxel_filter(all_pcds, all_colors, voxel_size=0.01)\n",
    "save_point_cloud(fused_pcd)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svlmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
